{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395f6590",
   "metadata": {},
   "source": [
    "# Dataset Column Header Conversion\n",
    "\n",
    "This notebook contains functions to convert the column headers in the meal planning dataset from the original format with units in parentheses to a simplified lowercase format without units.\n",
    "\n",
    "## Original Format:\n",
    "\n",
    "- `_id, Food_Item, Category, Calories (kcal), Protein (g), Carbohydrates (g), Fat (g), Fiber (g), Sugars (g), Sodium (mg), Cholesterol (mg), Meal_Type, Water_Intake (ml)`\n",
    "\n",
    "## New Format:\n",
    "\n",
    "- `id, food_item, category, calories, proteins, carbohydrates, fats, fibers, sugars, sodium, cholesterol, meal_type, water_intake`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0179c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def clean_dataset_with_mapping(input_file_path, output_file_path=None):\n",
    "    \"\"\"\n",
    "    Clean dataset by applying the specified column mapping and removing unspecified columns.\n",
    "    \n",
    "    Target columns: food_item, category, calories, proteins, carbohydrates, fats, \n",
    "                   fibers, sugars, sodium, cholesterol, meal_type, water_intake\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the standardized column mapping - removing _id and only keeping specified columns\n",
    "    column_mapping = {\n",
    "        'Food_Item': 'food_item',\n",
    "        'Category': 'category', \n",
    "        'Calories (kcal)': 'calories',\n",
    "        'Protein (g)': 'proteins',\n",
    "        'Carbohydrates (g)': 'carbohydrates',\n",
    "        'Fat (g)': 'fats',\n",
    "        'Fiber (g)': 'fibers',\n",
    "        'Sugars (g)': 'sugars',\n",
    "        'Sodium (mg)': 'sodium',\n",
    "        'Cholesterol (mg)': 'cholesterol',\n",
    "        'Meal_Type': 'meal_type',\n",
    "        'Water_Intake (ml)': 'water_intake'\n",
    "    }\n",
    "    \n",
    "    # Additional mappings for different dataset formats\n",
    "    extended_mapping = {\n",
    "        # Food_1.csv format\n",
    "        'name': 'food_item',\n",
    "        'calories': 'calories',\n",
    "        'protein': 'proteins',\n",
    "        'carbohydrates': 'carbohydrates',\n",
    "        'fat': 'fats',\n",
    "        'fiber': 'fibers',\n",
    "        'sugar': 'sugars',\n",
    "        \n",
    "        # Food_3.csv format\n",
    "        'Food': 'food_item',\n",
    "        'Calories-kcl': 'calories',\n",
    "        'Protein-g': 'proteins',\n",
    "        'Carb-g': 'carbohydrates',\n",
    "        'Fiber-g': 'fibers',\n",
    "        'Sugar-g': 'sugars',\n",
    "        'Sodium-g': 'sodium',\n",
    "        \n",
    "        # Food_4.csv format\n",
    "        'Shrt_Desc': 'food_item',\n",
    "        'Energ_Kcal': 'calories',\n",
    "        'Protein_(g)': 'proteins',\n",
    "        'Carbohydrt_(g)': 'carbohydrates',\n",
    "        'Lipid_Tot_(g)': 'fats',\n",
    "        'Fiber_TD_(g)': 'fibers',\n",
    "        'Sugar_Tot_(g)': 'sugars',\n",
    "        'Sodium_(mg)': 'sodium',\n",
    "        'Cholestrl_(mg)': 'cholesterol',\n",
    "        \n",
    "        # Food_5.csv format\n",
    "        'name': 'food_item',\n",
    "        'nutri_energy': 'calories',\n",
    "        'nutri_protein': 'proteins',\n",
    "        'nutri_carbohydrate': 'carbohydrates',\n",
    "        'nutri_fat': 'fats',\n",
    "        'nutri_fiber': 'fibers',\n",
    "        'nutri_sugar': 'sugars',\n",
    "        'nutri_salt': 'sodium'\n",
    "    }\n",
    "    \n",
    "    # Combine all mappings\n",
    "    all_mappings = {**column_mapping, **extended_mapping}\n",
    "    \n",
    "    # Target columns we want to keep\n",
    "    target_columns = list(column_mapping.values())\n",
    "    \n",
    "    # Read the CSV file with encoding handling\n",
    "    print(f\"Reading file: {input_file_path}\")\n",
    "    \n",
    "    # Try different encodings\n",
    "    encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
    "    df = None\n",
    "    \n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            df = pd.read_csv(input_file_path, encoding=encoding)\n",
    "            print(f\"Successfully read with encoding: {encoding}\")\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    \n",
    "    if df is None:\n",
    "        raise ValueError(f\"Could not read file {input_file_path} with any of the attempted encodings: {encodings_to_try}\")\n",
    "    \n",
    "    print(f\"Original columns: {list(df.columns)}\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Apply column mapping\n",
    "    df_renamed = df.rename(columns=all_mappings)\n",
    "    \n",
    "    # Keep only the target columns that exist in the dataset\n",
    "    available_target_cols = [col for col in target_columns if col in df_renamed.columns]\n",
    "    df_cleaned = df_renamed[available_target_cols].copy()\n",
    "    \n",
    "    print(f\"Available target columns: {available_target_cols}\")\n",
    "    print(f\"Cleaned dataset shape: {df_cleaned.shape}\")\n",
    "    \n",
    "    # Set output file path if not provided\n",
    "    if output_file_path is None:\n",
    "        base_name = os.path.splitext(os.path.basename(input_file_path))[0]\n",
    "        output_dir = os.path.dirname(input_file_path)\n",
    "        output_file_path = os.path.join(output_dir, f\"{base_name}_cleaned.csv\")\n",
    "    \n",
    "    # Save the cleaned dataset\n",
    "    df_cleaned.to_csv(output_file_path, index=False)\n",
    "    print(f\"Cleaned dataset saved to: {output_file_path}\")\n",
    "    print(f\"Final columns: {list(df_cleaned.columns)}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "def batch_clean_datasets(folder_path):\n",
    "    \"\"\"\n",
    "    Clean all CSV files in the specified folder using the standardized column mapping.\n",
    "    \"\"\"\n",
    "    print(f\"Starting batch cleaning of datasets in: {folder_path}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Find all CSV files in the folder\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv') and not f.endswith('_cleaned.csv')]\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found in the specified folder.\")\n",
    "        return\n",
    "    \n",
    "    cleaned_datasets = {}\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        input_path = os.path.join(folder_path, csv_file)\n",
    "        print(f\"\\nProcessing: {csv_file}\")\n",
    "        \n",
    "        try:\n",
    "            cleaned_df = clean_dataset_with_mapping(input_path)\n",
    "            cleaned_datasets[csv_file] = cleaned_df\n",
    "            print(f\"✓ Successfully cleaned {csv_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error cleaning {csv_file}: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BATCH CLEANING SUMMARY:\")\n",
    "    print(f\"Total files processed: {len(cleaned_datasets)}\")\n",
    "    \n",
    "    for filename, df in cleaned_datasets.items():\n",
    "        print(f\"- {filename}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "    \n",
    "    return cleaned_datasets\n",
    "\n",
    "def combine_all_cleaned_datasets(folder_path, output_filename='combined_food_dataset.csv'):\n",
    "    \"\"\"\n",
    "    Combine all cleaned datasets into one unified dataset.\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    \n",
    "    # Find all cleaned CSV files\n",
    "    cleaned_files = glob.glob(os.path.join(folder_path, '*_cleaned.csv'))\n",
    "    \n",
    "    if not cleaned_files:\n",
    "        print(\"No cleaned files found. Please run the cleaning process first.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(cleaned_files)} cleaned files to combine:\")\n",
    "    for file in cleaned_files:\n",
    "        print(f\"  - {os.path.basename(file)}\")\n",
    "    \n",
    "    combined_dfs = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    # Read and combine all cleaned files\n",
    "    for file_path in cleaned_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"\\nReading {filename}...\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Add source file information\n",
    "            df['source_file'] = filename.replace('_cleaned.csv', '.csv')\n",
    "            combined_dfs.append(df)\n",
    "            total_rows += len(df)\n",
    "            print(f\"  ✓ Added {len(df)} rows from {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error reading {filename}: {str(e)}\")\n",
    "    \n",
    "    if not combined_dfs:\n",
    "        print(\"No data could be read from cleaned files.\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    print(f\"\\nCombining {len(combined_dfs)} datasets...\")\n",
    "    combined_df = pd.concat(combined_dfs, ignore_index=True)\n",
    "    \n",
    "    # Reorder columns to put source_file at the end\n",
    "    cols = [col for col in combined_df.columns if col != 'source_file'] + ['source_file']\n",
    "    combined_df = combined_df[cols]\n",
    "    \n",
    "    # Save combined dataset\n",
    "    output_path = os.path.join(folder_path, output_filename)\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\n🎉 COMBINED DATASET CREATED:\")\n",
    "    print(f\"  📁 File: {output_path}\")\n",
    "    print(f\"  📊 Total rows: {len(combined_df):,}\")\n",
    "    print(f\"  📋 Columns: {list(combined_df.columns)}\")\n",
    "    print(f\"  🔗 Source files: {combined_df['source_file'].unique()}\")\n",
    "    \n",
    "    # Show distribution by source file\n",
    "    print(f\"\\n📈 DISTRIBUTION BY SOURCE:\")\n",
    "    source_counts = combined_df['source_file'].value_counts()\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"  - {source}: {count:,} rows ({count/len(combined_df)*100:.1f}%)\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b272aea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch cleaning of all datasets in the childs folder...\n",
      "This will apply the standardized column mapping and remove any unspecified columns.\n",
      "\n",
      "Target columns: food_item, category, calories, proteins, carbohydrates, fats, fibers, sugars, sodium, cholesterol, meal_type, water_intake\n",
      "\n",
      "================================================================================\n",
      "Starting batch cleaning of datasets in: ../../dataset/childs/\n",
      "============================================================\n",
      "\n",
      "Processing: Food_1.csv\n",
      "Reading file: ../../dataset/childs/Food_1.csv\n",
      "Successfully read with encoding: utf-8\n",
      "Original columns: ['Unnamed: 0', 'name', 'cook_time_minutes', 'country', 'user_ratings', 'description', 'fiber', 'protein', 'fat', 'calories', 'sugar', 'carbohydrates']\n",
      "Dataset shape: (38, 12)\n",
      "Available target columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars']\n",
      "Cleaned dataset shape: (38, 7)\n",
      "Cleaned dataset saved to: ../../dataset/childs\\Food_1_cleaned.csv\n",
      "Final columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars']\n",
      "==================================================\n",
      "✓ Successfully cleaned Food_1.csv\n",
      "\n",
      "Processing: Food_2.csv\n",
      "Reading file: ../../dataset/childs/Food_2.csv\n",
      "Successfully read with encoding: utf-8\n",
      "Original columns: ['Date', 'User_ID', 'Food_Item', 'Category', 'Calories (kcal)', 'Protein (g)', 'Carbohydrates (g)', 'Fat (g)', 'Fiber (g)', 'Sugars (g)', 'Sodium (mg)', 'Cholesterol (mg)', 'Meal_Type', 'Water_Intake (ml)']\n",
      "Dataset shape: (10000, 14)\n",
      "Available target columns: ['food_item', 'category', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium', 'cholesterol', 'meal_type', 'water_intake']\n",
      "Cleaned dataset shape: (10000, 12)\n",
      "Cleaned dataset saved to: ../../dataset/childs\\Food_2_cleaned.csv\n",
      "Final columns: ['food_item', 'category', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium', 'cholesterol', 'meal_type', 'water_intake']\n",
      "==================================================\n",
      "✓ Successfully cleaned Food_2.csv\n",
      "\n",
      "Processing: Food_3.csv\n",
      "Reading file: ../../dataset/childs/Food_3.csv\n",
      "Successfully read with encoding: utf-8\n",
      "Original columns: ['Food', 'ProductType', 'FlavorVariant', 'Calories-kcl', 'Protein-g', 'Carb-g', 'Fiber-g', 'Sugar-g', 'Sodium-g', 'SaturatedFat-g', 'MonounsaturatedFat-g', 'PolyunsaturatedFat-g', 'TransaFat-g', 'MeasureQuantity', 'MeasureType', 'MQCalories-kcl', 'MQGmWt', 'MQCalories100gm-kcl']\n",
      "Dataset shape: (656, 18)\n",
      "Available target columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fibers', 'sugars', 'sodium']\n",
      "Cleaned dataset shape: (656, 7)\n",
      "Cleaned dataset saved to: ../../dataset/childs\\Food_3_cleaned.csv\n",
      "Final columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fibers', 'sugars', 'sodium']\n",
      "==================================================\n",
      "✓ Successfully cleaned Food_3.csv\n",
      "\n",
      "Processing: Food_4.csv\n",
      "Reading file: ../../dataset/childs/Food_4.csv\n",
      "Successfully read with encoding: latin-1\n",
      "Original columns: ['NDB_No', 'Shrt_Desc', 'Water_(g)', 'Energ_Kcal', 'Protein_(g)', 'Lipid_Tot_(g)', 'Ash_(g)', 'Carbohydrt_(g)', 'Fiber_TD_(g)', 'Sugar_Tot_(g)', 'Calcium_(mg)', 'Iron_(mg)', 'Magnesium_(mg)', 'Phosphorus_(mg)', 'Potassium_(mg)', 'Sodium_(mg)', 'Zinc_(mg)', 'Copper_mg)', 'Manganese_(mg)', 'Selenium_(¾g)', 'Vit_C_(mg)', 'Thiamin_(mg)', 'Riboflavin_(mg)', 'Niacin_(mg)', 'Panto_Acid_mg)', 'Vit_B6_(mg)', 'Folate_Tot_(¾g)', 'Folic_Acid_(¾g)', 'Food_Folate_(¾g)', 'Folate_DFE_(¾g)', 'Choline_Tot_ (mg)', 'Vit_B12_(¾g)', 'Vit_A_IU', 'Vit_A_RAE', 'Retinol_(¾g)', 'Alpha_Carot_(¾g)', 'Beta_Carot_(¾g)', 'Beta_Crypt_(¾g)', 'Lycopene_(¾g)', 'Lut+Zea_ (¾g)', 'Vit_E_(mg)', 'Vit_D_¾g', 'Vit_D_IU', 'Vit_K_(¾g)', 'FA_Sat_(g)', 'FA_Mono_(g)', 'FA_Poly_(g)', 'Cholestrl_(mg)', 'GmWt_1', 'GmWt_Desc1', 'GmWt_2', 'GmWt_Desc2']\n",
      "Dataset shape: (8790, 52)\n",
      "Available target columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium', 'cholesterol']\n",
      "Cleaned dataset shape: (8790, 9)\n",
      "Cleaned dataset saved to: ../../dataset/childs\\Food_4_cleaned.csv\n",
      "Final columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium', 'cholesterol']\n",
      "==================================================\n",
      "✓ Successfully cleaned Food_4.csv\n",
      "\n",
      "Processing: Food_5.csv\n",
      "Reading file: ../../dataset/childs/Food_5.csv\n",
      "Successfully read with encoding: utf-8\n",
      "Original columns: ['food_link', 'name', 'brand', 'nutri_score', 'processing_score', 'nutri_energy', 'nutri_fat', 'nutri_satuFat', 'nutri_carbohydrate', 'nutri_sugar', 'nutri_fiber', 'nutri_protein', 'nutri_salt']\n",
      "Dataset shape: (1460, 13)\n",
      "Available target columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium']\n",
      "Cleaned dataset shape: (1460, 8)\n",
      "Cleaned dataset saved to: ../../dataset/childs\\Food_5_cleaned.csv\n",
      "Final columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium']\n",
      "==================================================\n",
      "✓ Successfully cleaned Food_5.csv\n",
      "\n",
      "============================================================\n",
      "BATCH CLEANING SUMMARY:\n",
      "Total files processed: 5\n",
      "- Food_1.csv: 38 rows, 7 columns\n",
      "  Columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars']\n",
      "- Food_2.csv: 10000 rows, 12 columns\n",
      "  Columns: ['food_item', 'category', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium', 'cholesterol', 'meal_type', 'water_intake']\n",
      "- Food_3.csv: 656 rows, 7 columns\n",
      "  Columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fibers', 'sugars', 'sodium']\n",
      "- Food_4.csv: 8790 rows, 9 columns\n",
      "  Columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium', 'cholesterol']\n",
      "- Food_5.csv: 1460 rows, 8 columns\n",
      "  Columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium']\n"
     ]
    }
   ],
   "source": [
    "# Execute batch cleaning on all files in the childs folder\n",
    "childs_folder_path = '../../dataset/childs/'\n",
    "\n",
    "print(\"Starting batch cleaning of all datasets in the childs folder...\")\n",
    "print(\"This will apply the standardized column mapping and remove any unspecified columns.\")\n",
    "print(\"\\nTarget columns: food_item, category, calories, proteins, carbohydrates, fats, fibers, sugars, sodium, cholesterol, meal_type, water_intake\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Run the batch cleaning\n",
    "cleaned_datasets = batch_clean_datasets(childs_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ccc35aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETAILED VERIFICATION OF CLEANED DATASETS:\n",
      "============================================================\n",
      "\n",
      "📁 Food_1.csv\n",
      "Shape: (38, 7)\n",
      "Columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars']\n",
      "\n",
      "First 3 rows:\n",
      "                           food_item  calories  proteins  carbohydrates  fats  fibers  sugars\n",
      "0           Tomato And Anchovy Pasta       755        24            109    22      19       9\n",
      "1            Blueberry Cream Muffins       264         4             32    13       0      13\n",
      "2  One-Pot Lemon Garlic Shrimp Pasta       678        38             49    37       4       2\n",
      "\n",
      "Data types:\n",
      "  food_item: object\n",
      "  calories: int64\n",
      "  proteins: int64\n",
      "  carbohydrates: int64\n",
      "  fats: int64\n",
      "  fibers: int64\n",
      "  sugars: int64\n",
      "\n",
      "✓ No missing values\n",
      "--------------------------------------------------\n",
      "\n",
      "📁 Food_2.csv\n",
      "Shape: (10000, 12)\n",
      "Columns: ['food_item', 'category', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium', 'cholesterol', 'meal_type', 'water_intake']\n",
      "\n",
      "First 3 rows:\n",
      "        food_item category  calories  proteins  carbohydrates  fats  fibers  sugars  sodium  cholesterol  meal_type  water_intake\n",
      "0            Eggs     Meat       173      42.4           83.7   1.5     1.5    12.7     752          125      Lunch           478\n",
      "1           Apple   Fruits        66      39.2           13.8   3.2     2.6    12.2     680           97      Lunch           466\n",
      "2  Chicken Breast     Meat       226      27.1           79.1  25.8     3.2    44.7     295          157  Breakfast           635\n",
      "\n",
      "Data types:\n",
      "  food_item: object\n",
      "  category: object\n",
      "  calories: int64\n",
      "  proteins: float64\n",
      "  carbohydrates: float64\n",
      "  fats: float64\n",
      "  fibers: float64\n",
      "  sugars: float64\n",
      "  sodium: int64\n",
      "  cholesterol: int64\n",
      "  meal_type: object\n",
      "  water_intake: int64\n",
      "\n",
      "✓ No missing values\n",
      "--------------------------------------------------\n",
      "\n",
      "📁 Food_3.csv\n",
      "Shape: (656, 7)\n",
      "Columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fibers', 'sugars', 'sodium']\n",
      "\n",
      "First 3 rows:\n",
      "  food_item  calories  proteins  carbohydrates  fibers  sugars  sodium\n",
      "0  Andrea'S       257       5.7           40.2     2.9     3.0     544\n",
      "1   Archway       497       4.3           65.0     2.0    34.1     270\n",
      "2   Archway       460       3.0           61.2     5.1    45.2     200\n",
      "\n",
      "Data types:\n",
      "  food_item: object\n",
      "  calories: int64\n",
      "  proteins: float64\n",
      "  carbohydrates: float64\n",
      "  fibers: float64\n",
      "  sugars: float64\n",
      "  sodium: int64\n",
      "\n",
      "✓ No missing values\n",
      "--------------------------------------------------\n",
      "\n",
      "📁 Food_4.csv\n",
      "Shape: (8790, 9)\n",
      "Columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium', 'cholesterol']\n",
      "\n",
      "First 3 rows:\n",
      "                food_item  calories  proteins  carbohydrates   fats  fibers  sugars  sodium  cholesterol\n",
      "0        BUTTER,WITH SALT       717      0.85           0.06  81.11     0.0    0.06   643.0        215.0\n",
      "1  BUTTER,WHIPPED,W/ SALT       718      0.49           2.87  78.30     0.0    0.06   583.0        225.0\n",
      "2    BUTTER OIL,ANHYDROUS       876      0.28           0.00  99.48     0.0    0.00     2.0        256.0\n",
      "\n",
      "Data types:\n",
      "  food_item: object\n",
      "  calories: int64\n",
      "  proteins: float64\n",
      "  carbohydrates: float64\n",
      "  fats: float64\n",
      "  fibers: float64\n",
      "  sugars: float64\n",
      "  sodium: float64\n",
      "  cholesterol: float64\n",
      "\n",
      "Missing values:\n",
      "  fibers: 594\n",
      "  sugars: 1832\n",
      "  sodium: 83\n",
      "  cholesterol: 410\n",
      "--------------------------------------------------\n",
      "\n",
      "📁 Food_5.csv\n",
      "Shape: (1460, 8)\n",
      "Columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium']\n",
      "\n",
      "First 3 rows:\n",
      "                                    food_item              calories proteins carbohydrates   fats fibers sugars sodium\n",
      "0  Mango and chilis pickles - Mother's receip      84 kj\\n(20 kcal)      NaN           1 g  1.5 g    NaN    NaN    NaN\n",
      "1                     Max Protein - Rite it’s  1,117 kj\\n(267 kcal)     20 g        14.8 g    NaN    5 g    NaN    NaN\n",
      "2                           Kamarkat - 100 gm  1,891 kj\\n(452 kcal)      2 g           NaN    NaN    NaN    NaN    NaN\n",
      "\n",
      "Data types:\n",
      "  food_item: object\n",
      "  calories: object\n",
      "  proteins: object\n",
      "  carbohydrates: object\n",
      "  fats: object\n",
      "  fibers: object\n",
      "  sugars: object\n",
      "  sodium: object\n",
      "\n",
      "Missing values:\n",
      "  calories: 214\n",
      "  proteins: 327\n",
      "  carbohydrates: 256\n",
      "  fats: 298\n",
      "  fibers: 732\n",
      "  sugars: 360\n",
      "  sodium: 422\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Verify the cleaned datasets\n",
    "if 'cleaned_datasets' in locals():\n",
    "    print(\"DETAILED VERIFICATION OF CLEANED DATASETS:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for filename, df in cleaned_datasets.items():\n",
    "        print(f\"\\n📁 {filename}\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Show first few rows\n",
    "        print(\"\\nFirst 3 rows:\")\n",
    "        print(df.head(3).to_string())\n",
    "        \n",
    "        # Check data types\n",
    "        print(\"\\nData types:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  {col}: {df[col].dtype}\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_counts = df.isnull().sum()\n",
    "        if missing_counts.sum() > 0:\n",
    "            print(\"\\nMissing values:\")\n",
    "            for col, count in missing_counts.items():\n",
    "                if count > 0:\n",
    "                    print(f\"  {col}: {count}\")\n",
    "        else:\n",
    "            print(\"\\n✓ No missing values\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "else:\n",
    "    print(\"No cleaned datasets found. Please run the batch cleaning first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f5a08e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample API format output:\n",
      "Using dataset: Food_1.csv\n",
      "Row 1: {'fats': 22.0, 'calories': 755.0, 'sugars': 9.0, 'proteins': 24.0, 'fibers': 19.0, 'sodium': 0.0, 'cholesterol': 0.0, 'carbohydrates': 109.0}\n",
      "Row 2: {'fats': 13.0, 'calories': 264.0, 'sugars': 13.0, 'proteins': 4.0, 'fibers': 0.0, 'sodium': 0.0, 'cholesterol': 0.0, 'carbohydrates': 32.0}\n",
      "Row 3: {'fats': 37.0, 'calories': 678.0, 'sugars': 2.0, 'proteins': 38.0, 'fibers': 4.0, 'sodium': 0.0, 'cholesterol': 0.0, 'carbohydrates': 49.0}\n"
     ]
    }
   ],
   "source": [
    "def convert_to_api_format(df, selected_rows=None):\n",
    "    \"\"\"\n",
    "    Convert dataframe rows to the API response format.\n",
    "    Returns only the nutritional data as specified in the API format.\n",
    "    \"\"\"\n",
    "    if selected_rows is not None:\n",
    "        df_subset = df.iloc[selected_rows]\n",
    "    else:\n",
    "        df_subset = df\n",
    "    \n",
    "    api_format_data = []\n",
    "    \n",
    "    for _, row in df_subset.iterrows():\n",
    "        item = {\n",
    "            \"fats\": float(row.get(\"fats\", 0)),\n",
    "            \"calories\": float(row.get(\"calories\", 0)),\n",
    "            \"sugars\": float(row.get(\"sugars\", 0)),\n",
    "            \"proteins\": float(row.get(\"proteins\", 0)),\n",
    "            \"fibers\": float(row.get(\"fibers\", 0)),\n",
    "            \"sodium\": float(row.get(\"sodium\", 0)),\n",
    "            \"cholesterol\": float(row.get(\"cholesterol\", 0)),\n",
    "            \"carbohydrates\": float(row.get(\"carbohydrates\", 0))\n",
    "        }\n",
    "        api_format_data.append(item)\n",
    "    \n",
    "    return api_format_data\n",
    "\n",
    "# Test the API format conversion with available cleaned datasets\n",
    "if 'cleaned_datasets' in locals() and cleaned_datasets:\n",
    "    print(\"Sample API format output:\")\n",
    "    # Use the first available dataset for testing\n",
    "    first_dataset_name = list(cleaned_datasets.keys())[0]\n",
    "    first_dataset = cleaned_datasets[first_dataset_name]\n",
    "    \n",
    "    print(f\"Using dataset: {first_dataset_name}\")\n",
    "    \n",
    "    # Test with first 3 rows if available\n",
    "    num_rows = min(3, len(first_dataset))\n",
    "    if num_rows > 0:\n",
    "        sample_api_data = convert_to_api_format(first_dataset, list(range(num_rows)))\n",
    "        for i, item in enumerate(sample_api_data):\n",
    "            print(f\"Row {i+1}: {item}\")\n",
    "    else:\n",
    "        print(\"No data available in the dataset.\")\n",
    "else:\n",
    "    print(\"No cleaned datasets available. Please run the batch cleaning first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de2696f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEANED FILES SAVED TO DISK:\n",
      "========================================\n",
      "\n",
      "📄 Food_1_cleaned.csv\n",
      "   Size: 1,839 bytes\n",
      "   Shape: (38, 7)\n",
      "   Columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars']\n",
      "   Sample row: {'food_item': 'Tomato And Anchovy Pasta', 'calories': 755, 'proteins': 24, 'carbohydrates': 109, 'fats': 22, 'fibers': 19, 'sugars': 9}\n",
      "\n",
      "📄 Food_2_cleaned.csv\n",
      "   Size: 624,010 bytes\n",
      "   Shape: (10000, 12)\n",
      "   Columns: ['food_item', 'category', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium', 'cholesterol', 'meal_type', 'water_intake']\n",
      "   Sample row: {'food_item': 'Eggs', 'category': 'Meat', 'calories': 173, 'proteins': 42.4, 'carbohydrates': 83.7, 'fats': 1.5, 'fibers': 1.5, 'sugars': 12.7, 'sodium': 752, 'cholesterol': 125, 'meal_type': 'Lunch', 'water_intake': 478}\n",
      "\n",
      "📄 Food_3_cleaned.csv\n",
      "   Size: 23,687 bytes\n",
      "   Shape: (656, 7)\n",
      "   Columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fibers', 'sugars', 'sodium']\n",
      "   Sample row: {'food_item': \"Andrea'S\", 'calories': 257, 'proteins': 5.7, 'carbohydrates': 40.2, 'fibers': 2.9, 'sugars': 3.0, 'sodium': 544}\n",
      "\n",
      "📄 Food_4_cleaned.csv\n",
      "   Size: 692,222 bytes\n",
      "   Shape: (8790, 9)\n",
      "   Columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium', 'cholesterol']\n",
      "   Sample row: {'food_item': 'BUTTER,WITH SALT', 'calories': 717, 'proteins': 0.85, 'carbohydrates': 0.06, 'fats': 81.11, 'fibers': 0.0, 'sugars': 0.06, 'sodium': 643.0, 'cholesterol': 215.0}\n",
      "\n",
      "📄 Food_5_cleaned.csv\n",
      "   Size: 107,539 bytes\n",
      "   Shape: (1460, 8)\n",
      "   Columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium']\n",
      "   Sample row: {'food_item': \"Mango and chilis pickles - Mother's receip\", 'calories': '84 kj\\n(20 kcal)', 'proteins': nan, 'carbohydrates': '1 g', 'fats': '1.5 g', 'fibers': nan, 'sugars': nan, 'sodium': nan}\n",
      "\n",
      "========================================\n",
      "CLEANING PROCESS COMPLETED SUCCESSFULLY! ✅\n",
      "All datasets have been standardized with the specified column mapping.\n",
      "Original files remain unchanged. Cleaned versions saved with '_cleaned' suffix.\n"
     ]
    }
   ],
   "source": [
    "# Check the cleaned files that were saved to disk\n",
    "import glob\n",
    "\n",
    "childs_folder = '../../dataset/childs/'\n",
    "cleaned_files = glob.glob(os.path.join(childs_folder, '*_cleaned.csv'))\n",
    "\n",
    "print(\"CLEANED FILES SAVED TO DISK:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if cleaned_files:\n",
    "    for file_path in cleaned_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        \n",
    "        # Read the file to check its structure\n",
    "        df_check = pd.read_csv(file_path)\n",
    "        \n",
    "        print(f\"\\n📄 {filename}\")\n",
    "        print(f\"   Size: {file_size:,} bytes\")\n",
    "        print(f\"   Shape: {df_check.shape}\")\n",
    "        print(f\"   Columns: {list(df_check.columns)}\")\n",
    "        \n",
    "        # Show sample of the data\n",
    "        if len(df_check) > 0:\n",
    "            print(f\"   Sample row: {df_check.iloc[0].to_dict()}\")\n",
    "else:\n",
    "    print(\"No cleaned files found on disk.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"CLEANING PROCESS COMPLETED SUCCESSFULLY! ✅\")\n",
    "print(\"All datasets have been standardized with the specified column mapping.\")\n",
    "print(\"Original files remain unchanged. Cleaned versions saved with '_cleaned' suffix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d16d38ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RE-RUNNING BATCH CLEANING WITH ENCODING FIX:\n",
      "============================================================\n",
      "Starting batch cleaning of datasets in: ../../dataset/childs/\n",
      "============================================================\n",
      "\n",
      "Processing: Food_1.csv\n",
      "Reading file: ../../dataset/childs/Food_1.csv\n",
      "Successfully read with encoding: utf-8\n",
      "Original columns: ['Unnamed: 0', 'name', 'cook_time_minutes', 'country', 'user_ratings', 'description', 'fiber', 'protein', 'fat', 'calories', 'sugar', 'carbohydrates']\n",
      "Dataset shape: (38, 12)\n",
      "Available target columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars']\n",
      "Cleaned dataset shape: (38, 7)\n",
      "Cleaned dataset saved to: ../../dataset/childs\\Food_1_cleaned.csv\n",
      "Final columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars']\n",
      "==================================================\n",
      "✓ Successfully cleaned Food_1.csv\n",
      "\n",
      "Processing: Food_2.csv\n",
      "Reading file: ../../dataset/childs/Food_2.csv\n",
      "Successfully read with encoding: utf-8\n",
      "Original columns: ['Date', 'User_ID', 'Food_Item', 'Category', 'Calories (kcal)', 'Protein (g)', 'Carbohydrates (g)', 'Fat (g)', 'Fiber (g)', 'Sugars (g)', 'Sodium (mg)', 'Cholesterol (mg)', 'Meal_Type', 'Water_Intake (ml)']\n",
      "Dataset shape: (10000, 14)\n",
      "Available target columns: ['food_item', 'category', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium', 'cholesterol', 'meal_type', 'water_intake']\n",
      "Cleaned dataset shape: (10000, 12)\n",
      "Cleaned dataset saved to: ../../dataset/childs\\Food_2_cleaned.csv\n",
      "Final columns: ['food_item', 'category', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium', 'cholesterol', 'meal_type', 'water_intake']\n",
      "==================================================\n",
      "✓ Successfully cleaned Food_2.csv\n",
      "\n",
      "Processing: Food_3.csv\n",
      "Reading file: ../../dataset/childs/Food_3.csv\n",
      "Successfully read with encoding: utf-8\n",
      "Original columns: ['Food', 'ProductType', 'FlavorVariant', 'Calories-kcl', 'Protein-g', 'Carb-g', 'Fiber-g', 'Sugar-g', 'Sodium-g', 'SaturatedFat-g', 'MonounsaturatedFat-g', 'PolyunsaturatedFat-g', 'TransaFat-g', 'MeasureQuantity', 'MeasureType', 'MQCalories-kcl', 'MQGmWt', 'MQCalories100gm-kcl']\n",
      "Dataset shape: (656, 18)\n",
      "Available target columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fibers', 'sugars', 'sodium']\n",
      "Cleaned dataset shape: (656, 7)\n",
      "Cleaned dataset saved to: ../../dataset/childs\\Food_3_cleaned.csv\n",
      "Final columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fibers', 'sugars', 'sodium']\n",
      "==================================================\n",
      "✓ Successfully cleaned Food_3.csv\n",
      "\n",
      "Processing: Food_4.csv\n",
      "Reading file: ../../dataset/childs/Food_4.csv\n",
      "Successfully read with encoding: latin-1\n",
      "Original columns: ['NDB_No', 'Shrt_Desc', 'Water_(g)', 'Energ_Kcal', 'Protein_(g)', 'Lipid_Tot_(g)', 'Ash_(g)', 'Carbohydrt_(g)', 'Fiber_TD_(g)', 'Sugar_Tot_(g)', 'Calcium_(mg)', 'Iron_(mg)', 'Magnesium_(mg)', 'Phosphorus_(mg)', 'Potassium_(mg)', 'Sodium_(mg)', 'Zinc_(mg)', 'Copper_mg)', 'Manganese_(mg)', 'Selenium_(¾g)', 'Vit_C_(mg)', 'Thiamin_(mg)', 'Riboflavin_(mg)', 'Niacin_(mg)', 'Panto_Acid_mg)', 'Vit_B6_(mg)', 'Folate_Tot_(¾g)', 'Folic_Acid_(¾g)', 'Food_Folate_(¾g)', 'Folate_DFE_(¾g)', 'Choline_Tot_ (mg)', 'Vit_B12_(¾g)', 'Vit_A_IU', 'Vit_A_RAE', 'Retinol_(¾g)', 'Alpha_Carot_(¾g)', 'Beta_Carot_(¾g)', 'Beta_Crypt_(¾g)', 'Lycopene_(¾g)', 'Lut+Zea_ (¾g)', 'Vit_E_(mg)', 'Vit_D_¾g', 'Vit_D_IU', 'Vit_K_(¾g)', 'FA_Sat_(g)', 'FA_Mono_(g)', 'FA_Poly_(g)', 'Cholestrl_(mg)', 'GmWt_1', 'GmWt_Desc1', 'GmWt_2', 'GmWt_Desc2']\n",
      "Dataset shape: (8790, 52)\n",
      "Available target columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium', 'cholesterol']\n",
      "Cleaned dataset shape: (8790, 9)\n",
      "Cleaned dataset saved to: ../../dataset/childs\\Food_4_cleaned.csv\n",
      "Final columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium', 'cholesterol']\n",
      "==================================================\n",
      "✓ Successfully cleaned Food_4.csv\n",
      "\n",
      "Processing: Food_5.csv\n",
      "Reading file: ../../dataset/childs/Food_5.csv\n",
      "Successfully read with encoding: utf-8\n",
      "Original columns: ['food_link', 'name', 'brand', 'nutri_score', 'processing_score', 'nutri_energy', 'nutri_fat', 'nutri_satuFat', 'nutri_carbohydrate', 'nutri_sugar', 'nutri_fiber', 'nutri_protein', 'nutri_salt']\n",
      "Dataset shape: (1460, 13)\n",
      "Available target columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium']\n",
      "Cleaned dataset shape: (1460, 8)\n",
      "Cleaned dataset saved to: ../../dataset/childs\\Food_5_cleaned.csv\n",
      "Final columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium']\n",
      "==================================================\n",
      "✓ Successfully cleaned Food_5.csv\n",
      "\n",
      "============================================================\n",
      "BATCH CLEANING SUMMARY:\n",
      "Total files processed: 5\n",
      "- Food_1.csv: 38 rows, 7 columns\n",
      "  Columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars']\n",
      "- Food_2.csv: 10000 rows, 12 columns\n",
      "  Columns: ['food_item', 'category', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium', 'cholesterol', 'meal_type', 'water_intake']\n",
      "- Food_3.csv: 656 rows, 7 columns\n",
      "  Columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fibers', 'sugars', 'sodium']\n",
      "- Food_4.csv: 8790 rows, 9 columns\n",
      "  Columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium', 'cholesterol']\n",
      "- Food_5.csv: 1460 rows, 8 columns\n",
      "  Columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium']\n",
      "\n",
      "============================================================\n",
      "CLEANING VERIFICATION:\n",
      "✅ Food_4.csv was successfully cleaned!\n",
      "   Shape: (8790, 9)\n",
      "   Columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium', 'cholesterol']\n",
      "   Sample data: [{'food_item': 'BUTTER,WITH SALT', 'calories': 717, 'proteins': 0.85, 'carbohydrates': 0.06, 'fats': 81.11, 'fibers': 0.0, 'sugars': 0.06, 'sodium': 643.0, 'cholesterol': 215.0}, {'food_item': 'BUTTER,WHIPPED,W/ SALT', 'calories': 718, 'proteins': 0.49, 'carbohydrates': 2.87, 'fats': 78.3, 'fibers': 0.0, 'sugars': 0.06, 'sodium': 583.0, 'cholesterol': 225.0}]\n"
     ]
    }
   ],
   "source": [
    "# Re-run the batch cleaning with encoding fix for Food_4.csv\n",
    "print(\"RE-RUNNING BATCH CLEANING WITH ENCODING FIX:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run the batch cleaning again\n",
    "cleaned_datasets = batch_clean_datasets(childs_folder_path)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLEANING VERIFICATION:\")\n",
    "\n",
    "# Check if Food_4.csv was processed successfully\n",
    "if 'Food_4.csv' in cleaned_datasets:\n",
    "    print(\"✅ Food_4.csv was successfully cleaned!\")\n",
    "    food4_df = cleaned_datasets['Food_4.csv']\n",
    "    print(f\"   Shape: {food4_df.shape}\")\n",
    "    print(f\"   Columns: {list(food4_df.columns)}\")\n",
    "    print(f\"   Sample data: {food4_df.head(2).to_dict('records')}\")\n",
    "else:\n",
    "    print(\"❌ Food_4.csv was not processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4eeda29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMBINING ALL CLEANED DATASETS INTO ONE UNIFIED DATASET\n",
      "================================================================================\n",
      "Found 5 cleaned files to combine:\n",
      "  - Food_1_cleaned.csv\n",
      "  - Food_2_cleaned.csv\n",
      "  - Food_3_cleaned.csv\n",
      "  - Food_4_cleaned.csv\n",
      "  - Food_5_cleaned.csv\n",
      "\n",
      "Reading Food_1_cleaned.csv...\n",
      "  ✓ Added 38 rows from Food_1_cleaned.csv\n",
      "\n",
      "Reading Food_2_cleaned.csv...\n",
      "  ✓ Added 10000 rows from Food_2_cleaned.csv\n",
      "\n",
      "Reading Food_3_cleaned.csv...\n",
      "  ✓ Added 656 rows from Food_3_cleaned.csv\n",
      "\n",
      "Reading Food_4_cleaned.csv...\n",
      "  ✓ Added 8790 rows from Food_4_cleaned.csv\n",
      "\n",
      "Reading Food_5_cleaned.csv...\n",
      "  ✓ Added 1460 rows from Food_5_cleaned.csv\n",
      "\n",
      "Combining 5 datasets...\n",
      "\n",
      "🎉 COMBINED DATASET CREATED:\n",
      "  📁 File: ../../dataset/childs/unified_food_dataset.csv\n",
      "  📊 Total rows: 20,944\n",
      "  📋 Columns: ['food_item', 'calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'category', 'sodium', 'cholesterol', 'meal_type', 'water_intake', 'source_file']\n",
      "  🔗 Source files: ['Food_1.csv' 'Food_2.csv' 'Food_3.csv' 'Food_4.csv' 'Food_5.csv']\n",
      "\n",
      "📈 DISTRIBUTION BY SOURCE:\n",
      "  - Food_2.csv: 10,000 rows (47.7%)\n",
      "  - Food_4.csv: 8,790 rows (42.0%)\n",
      "  - Food_5.csv: 1,460 rows (7.0%)\n",
      "  - Food_3.csv: 656 rows (3.1%)\n",
      "  - Food_1.csv: 38 rows (0.2%)\n",
      "\n",
      "🔍 UNIFIED DATASET ANALYSIS:\n",
      "Total unique food items: 10372\n",
      "\n",
      "📊 NUTRITIONAL SUMMARY STATISTICS:\n",
      "       cholesterol\n",
      "count     18380.00\n",
      "mean        101.15\n",
      "std         117.36\n",
      "min           0.00\n",
      "25%           6.00\n",
      "50%          75.00\n",
      "75%         171.00\n",
      "max        3100.00\n",
      "\n",
      "📄 SAMPLE OF UNIFIED DATASET (First 5 rows):\n",
      "                           food_item calories proteins carbohydrates fats fibers sugars category sodium  cholesterol meal_type  water_intake source_file\n",
      "0           Tomato And Anchovy Pasta      755       24           109   22     19      9      NaN    NaN          NaN       NaN           NaN  Food_1.csv\n",
      "1            Blueberry Cream Muffins      264        4            32   13      0     13      NaN    NaN          NaN       NaN           NaN  Food_1.csv\n",
      "2  One-Pot Lemon Garlic Shrimp Pasta      678       38            49   37      4      2      NaN    NaN          NaN       NaN           NaN  Food_1.csv\n",
      "3      One-Pot Garlic Parmesan Pasta      334       13            49   10      1      6      NaN    NaN          NaN       NaN           NaN  Food_1.csv\n",
      "4                 Chocolate Mug Cake      500        8            72   20      4     36      NaN    NaN          NaN       NaN           NaN  Food_1.csv\n",
      "\n",
      "✨ SUCCESS! All datasets have been cleaned and combined into one unified dataset.\n"
     ]
    }
   ],
   "source": [
    "# Combine all cleaned datasets into one unified dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMBINING ALL CLEANED DATASETS INTO ONE UNIFIED DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine all cleaned datasets\n",
    "combined_dataset = combine_all_cleaned_datasets(childs_folder_path, 'unified_food_dataset.csv')\n",
    "\n",
    "if combined_dataset is not None:\n",
    "    print(\"\\n🔍 UNIFIED DATASET ANALYSIS:\")\n",
    "    print(f\"Total unique food items: {combined_dataset['food_item'].nunique() if 'food_item' in combined_dataset.columns else 'N/A'}\")\n",
    "    \n",
    "    # Show summary statistics for nutritional columns\n",
    "    nutrition_cols = ['calories', 'proteins', 'carbohydrates', 'fats', 'fibers', 'sugars', 'sodium', 'cholesterol']\n",
    "    available_nutrition_cols = [col for col in nutrition_cols if col in combined_dataset.columns]\n",
    "    \n",
    "    if available_nutrition_cols:\n",
    "        print(f\"\\n📊 NUTRITIONAL SUMMARY STATISTICS:\")\n",
    "        print(combined_dataset[available_nutrition_cols].describe().round(2))\n",
    "    \n",
    "    # Show sample of the unified dataset\n",
    "    print(f\"\\n📄 SAMPLE OF UNIFIED DATASET (First 5 rows):\")\n",
    "    print(combined_dataset.head().to_string())\n",
    "    \n",
    "    print(\"\\n✨ SUCCESS! All datasets have been cleaned and combined into one unified dataset.\")\n",
    "else:\n",
    "    print(\"❌ Failed to create unified dataset.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
