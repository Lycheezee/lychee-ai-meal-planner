{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326a6397",
   "metadata": {},
   "source": [
    "# 05. Model Visualization and Feature Space Analysis\n",
    "\n",
    "This notebook creates advanced visualizations to understand food relationships in feature space and analyze model behavior through dimensionality reduction techniques.\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "- Visualize food relationships using PCA and t-SNE\n",
    "- Analyze feature importance and model interpretability\n",
    "- Create prediction confidence and error analysis\n",
    "- Understand food clustering patterns in nutritional space\n",
    "\n",
    "**Prerequisites**: Run notebooks 01-04 first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9bed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "from itertools import cycle\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(10)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "print(\"üé® Visualization libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3242c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model data and results\n",
    "print(\"üìÇ Loading Model Data and Analysis Results\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "try:\n",
    "    # Load data from previous notebooks\n",
    "    X_scaled = joblib.load('../models/X_scaled.pkl')\n",
    "    food_lookup = joblib.load('../models/food_lookup.pkl')\n",
    "    eval_data = joblib.load('../models/eval_subset.pkl')\n",
    "    final_model = joblib.load('../models/optimized_similarity_model.pkl')\n",
    "    model_config = joblib.load('../models/model_config.pkl')\n",
    "    similarity_results = joblib.load('../models/similarity_analysis_results.pkl')\n",
    "    \n",
    "    X_eval = eval_data['X_eval']\n",
    "    food_eval = eval_data['food_eval']\n",
    "    best_params = model_config['best_params']\n",
    "    feature_columns = model_config['feature_columns']\n",
    "    \n",
    "    print(f\"‚úÖ Loaded data: {X_scaled.shape} samples, {len(feature_columns)} features\")\n",
    "    print(f\"‚úÖ Evaluation subset: {len(X_eval)} samples\")\n",
    "    print(f\"‚úÖ Food categories: {len(food_lookup['category'].unique())} categories\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    print(\"Please run the previous notebooks first (01-04)\")\n",
    "    raise\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dadd895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Food Nutritional Space Visualization\n",
    "print(\"\\n1Ô∏è‚É£ FOOD NUTRITIONAL SPACE VISUALIZATION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create sample for visualization (subset for performance)\n",
    "sample_size = min(1000, len(X_scaled))\n",
    "sample_indices = np.random.choice(len(X_scaled), sample_size, replace=False)\n",
    "X_sample = X_scaled[sample_indices]\n",
    "food_sample = food_lookup.iloc[sample_indices].reset_index(drop=True)\n",
    "\n",
    "# Apply dimensionality reduction techniques\n",
    "print(\"Applying dimensionality reduction...\")\n",
    "\n",
    "# PCA\n",
    "pca_viz = PCA(n_components=2)\n",
    "X_pca = pca_viz.fit_transform(X_sample)\n",
    "\n",
    "# t-SNE  \n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_sample)\n",
    "\n",
    "# Create category color mapping\n",
    "unique_categories = food_sample['category'].unique()\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(unique_categories)))\n",
    "color_map = dict(zip(unique_categories, colors))\n",
    "category_colors = [color_map[cat] for cat in food_sample['category']]\n",
    "\n",
    "# Create label encoder for numerical representation\n",
    "label_encoder = LabelEncoder()\n",
    "y_sample = label_encoder.fit_transform(food_sample['category'])\n",
    "\n",
    "# Get predictions from final model\n",
    "best_model_pred = final_model.predict(X_sample)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# PCA - Food categories\n",
    "scatter1 = axes[0,0].scatter(X_pca[:, 0], X_pca[:, 1], c=category_colors, alpha=0.6)\n",
    "axes[0,0].set_title('PCA - Food Categories in Nutritional Space')\n",
    "axes[0,0].set_xlabel(f'PC1 ({pca_viz.explained_variance_ratio_[0]:.2%} variance)')\n",
    "axes[0,0].set_ylabel(f'PC2 ({pca_viz.explained_variance_ratio_[1]:.2%} variance)')\n",
    "\n",
    "# Add legend for categories (show first 10 for readability)\n",
    "legend_categories = unique_categories[:10]\n",
    "legend_colors = [color_map[cat] for cat in legend_categories]\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                             markerfacecolor=color, markersize=8, label=cat) \n",
    "                  for cat, color in zip(legend_categories, legend_colors)]\n",
    "axes[0,0].legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.3, 1))\n",
    "\n",
    "# PCA - Best Model Predictions\n",
    "scatter2 = axes[0,1].scatter(X_pca[:, 0], X_pca[:, 1], c=best_model_pred, cmap='tab10', alpha=0.6)\n",
    "axes[0,1].set_title('PCA - Model Predictions')\n",
    "axes[0,1].set_xlabel(f'PC1 ({pca_viz.explained_variance_ratio_[0]:.2%} variance)')\n",
    "axes[0,1].set_ylabel(f'PC2 ({pca_viz.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.colorbar(scatter2, ax=axes[0,1])\n",
    "\n",
    "# t-SNE - True labels\n",
    "scatter3 = axes[1,0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_sample, cmap='tab10', alpha=0.6)\n",
    "axes[1,0].set_title('t-SNE - True Food Categories')\n",
    "axes[1,0].set_xlabel('t-SNE Component 1')\n",
    "axes[1,0].set_ylabel('t-SNE Component 2')\n",
    "plt.colorbar(scatter3, ax=axes[1,0])\n",
    "\n",
    "# t-SNE - Best Model Predictions\n",
    "scatter4 = axes[1,1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=best_model_pred, cmap='tab10', alpha=0.6)\n",
    "axes[1,1].set_title('t-SNE - Model Predictions')\n",
    "axes[1,1].set_xlabel('t-SNE Component 1')\n",
    "axes[1,1].set_ylabel('t-SNE Component 2')\n",
    "plt.colorbar(scatter4, ax=axes[1,1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Feature space visualization completed!\")\n",
    "print(f\"üìä PCA explains {pca_viz.explained_variance_ratio_.sum():.2%} of total variance\")\n",
    "print(f\"üìä Visualized {sample_size} samples from {len(unique_categories)} food categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706b919",
   "metadata": {},
   "source": [
    "## Why Feature Space Visualization?\n",
    "\n",
    "**Purpose**: Understand how food items cluster in the feature space and evaluate model decision boundaries.\n",
    "\n",
    "**Why This Matters**:\n",
    "\n",
    "- **Data Distribution**: Visualize how different food categories are distributed in nutritional space\n",
    "- **Model Behavior**: See how KNN models make decisions based on local neighborhoods\n",
    "- **Clustering Patterns**: Identify natural groupings of similar foods\n",
    "- **Decision Boundary Analysis**: Compare true labels vs model predictions visually\n",
    "\n",
    "**Techniques Used**:\n",
    "\n",
    "- **PCA (Principal Component Analysis)**: Linear dimensionality reduction preserving maximum variance\n",
    "  - Good for understanding overall data structure\n",
    "  - Shows which nutritional combinations explain most variation\n",
    "- **t-SNE (t-Distributed Stochastic Neighbor Embedding)**: Non-linear reduction preserving local structure\n",
    "  - Better for visualizing clusters and local neighborhoods\n",
    "  - Reveals hidden patterns in high-dimensional nutritional data\n",
    "\n",
    "**What We Learn**:\n",
    "\n",
    "- Whether food categories form distinct clusters\n",
    "- How well KNN models capture these natural groupings\n",
    "- Potential misclassification patterns\n",
    "- Data quality issues (outliers, overlapping categories)\n",
    "\n",
    "**Business Value**: Understanding food similarity patterns helps improve meal planning algorithms and identify opportunities for better categorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f8a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model Prediction Confidence and Error Analysis\n",
    "print(\"\\n2Ô∏è‚É£ PREDICTION CONFIDENCE & ERROR ANALYSIS\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Create a test set for confidence analysis using evaluation data\n",
    "X_test_scaled = X_eval.values\n",
    "y_test = label_encoder.fit_transform(food_eval['category'])\n",
    "\n",
    "# Get prediction probabilities from best model\n",
    "best_model_proba = final_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Calculate prediction confidence (max probability)\n",
    "conf = np.max(best_model_proba, axis=1)\n",
    "\n",
    "# Identify correct and incorrect predictions\n",
    "correct = (final_model.predict(X_test_scaled) == y_test)\n",
    "\n",
    "# Create confidence analysis visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Confidence distribution for correct vs incorrect predictions\n",
    "axes[0,0].hist(conf[correct], bins=30, alpha=0.7, label='Correct', color='green')\n",
    "axes[0,0].hist(conf[~correct], bins=30, alpha=0.7, label='Incorrect', color='red')\n",
    "axes[0,0].set_title('Prediction Confidence Distribution')\n",
    "axes[0,0].set_xlabel('Confidence (Max Probability)')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Confidence vs Accuracy relationship (calibration curve)\n",
    "confidence_bins = np.linspace(0, 1, 11)\n",
    "bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2\n",
    "\n",
    "accuracies = []\n",
    "for i in range(len(confidence_bins)-1):\n",
    "    mask = (conf >= confidence_bins[i]) & (conf < confidence_bins[i+1])\n",
    "    if np.sum(mask) > 0:\n",
    "        accuracies.append(np.mean(correct[mask]))\n",
    "    else:\n",
    "        accuracies.append(0)\n",
    "\n",
    "axes[0,1].plot(bin_centers, accuracies, 'o-', label='Model Calibration', color='blue')\n",
    "axes[0,1].plot([0, 1], [0, 1], '--', color='gray', alpha=0.7, label='Perfect Calibration')\n",
    "axes[0,1].set_title('Model Calibration (Confidence vs Accuracy)')\n",
    "axes[0,1].set_xlabel('Prediction Confidence')\n",
    "axes[0,1].set_ylabel('Actual Accuracy')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Error analysis by food category\n",
    "error_analysis = pd.DataFrame({\n",
    "    'Category': label_encoder.classes_,\n",
    "    'Errors': [np.sum((y_test == i) & ~correct) for i in range(len(label_encoder.classes_))],\n",
    "    'Total_Samples': [np.sum(y_test == i) for i in range(len(label_encoder.classes_))]\n",
    "})\n",
    "\n",
    "error_analysis['Error_Rate'] = error_analysis['Errors'] / error_analysis['Total_Samples']\n",
    "error_analysis = error_analysis.sort_values('Error_Rate', ascending=True)\n",
    "\n",
    "# Show top 15 categories with most errors\n",
    "top_error_categories = error_analysis.tail(15)\n",
    "axes[1,0].barh(range(len(top_error_categories)), top_error_categories['Error_Rate'], \n",
    "               color='lightcoral', alpha=0.8)\n",
    "axes[1,0].set_yticks(range(len(top_error_categories)))\n",
    "axes[1,0].set_yticklabels(top_error_categories['Category'], fontsize=9)\n",
    "axes[1,0].set_title('Error Rate by Food Category (Top 15)')\n",
    "axes[1,0].set_xlabel('Error Rate')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Confidence statistics by category\n",
    "conf_stats = pd.DataFrame({\n",
    "    'Category': label_encoder.classes_,\n",
    "    'Avg_Confidence': [np.mean(conf[y_test == i]) if np.sum(y_test == i) > 0 else 0 for i in range(len(label_encoder.classes_))],\n",
    "    'Std_Confidence': [np.std(conf[y_test == i]) if np.sum(y_test == i) > 0 else 0 for i in range(len(label_encoder.classes_))]\n",
    "})\n",
    "\n",
    "conf_stats = conf_stats.sort_values('Avg_Confidence', ascending=False).head(15)\n",
    "\n",
    "axes[1,1].barh(range(len(conf_stats)), conf_stats['Avg_Confidence'], \n",
    "               xerr=conf_stats['Std_Confidence'], capsize=3,\n",
    "               color='skyblue', alpha=0.8)\n",
    "axes[1,1].set_yticks(range(len(conf_stats)))\n",
    "axes[1,1].set_yticklabels(conf_stats['Category'], fontsize=9)\n",
    "axes[1,1].set_title('Average Confidence by Food Category (Top 15)')\n",
    "axes[1,1].set_xlabel('Average Confidence')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Average Prediction Confidence: {np.mean(conf):.3f}\")\n",
    "print(f\"üìä High Confidence Predictions (>0.8): {np.sum(conf > 0.8)}/{len(conf)} ({np.mean(conf > 0.8):.1%})\")\n",
    "print(f\"üìä Low Confidence Predictions (<0.5): {np.sum(conf < 0.5)}/{len(conf)} ({np.mean(conf < 0.5):.1%})\")\n",
    "\n",
    "# Show categories with highest error rates\n",
    "worst_categories = error_analysis.nlargest(3, 'Error_Rate')[['Category', 'Error_Rate', 'Total_Samples']]\n",
    "print(f\"\\n‚ö†Ô∏è Categories with Highest Error Rates:\")\n",
    "for _, row in worst_categories.iterrows():\n",
    "    print(f\"   {row['Category']}: {row['Error_Rate']:.2%} ({row['Total_Samples']} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20015112",
   "metadata": {},
   "source": [
    "## Why Prediction Confidence & Error Analysis?\n",
    "\n",
    "**Purpose**: Evaluate model reliability and understand prediction uncertainty for practical deployment.\n",
    "\n",
    "**Why This Matters**:\n",
    "\n",
    "- **Confidence Calibration**: Know when the model is uncertain vs confident\n",
    "- **Error Pattern Analysis**: Identify systematic weaknesses in model predictions\n",
    "- **Category-Specific Performance**: Some food categories may be harder to classify\n",
    "- **Production Reliability**: Understand when to trust model predictions\n",
    "\n",
    "**What We Analyze**:\n",
    "\n",
    "- **Confidence Distribution**: How often is the model confident vs uncertain?\n",
    "- **Calibration Curves**: Does high confidence actually mean high accuracy?\n",
    "- **Error by Category**: Which food types cause the most classification errors?\n",
    "- **Confidence Thresholds**: What confidence level should trigger manual review?\n",
    "\n",
    "**Key Insights**:\n",
    "\n",
    "- **Well-calibrated models**: High confidence should correlate with high accuracy\n",
    "- **Error patterns**: Reveal data quality issues or feature limitations\n",
    "- **Threshold setting**: Balance automation vs manual oversight\n",
    "\n",
    "**Business Value**:\n",
    "\n",
    "- Set appropriate confidence thresholds for automated meal planning\n",
    "- Identify categories needing human review\n",
    "- Improve user trust through transparent uncertainty communication\n",
    "- Optimize the balance between automation and accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f90a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Importance and Model Interpretability\n",
    "print(\"\\n3Ô∏è‚É£ FEATURE IMPORTANCE & MODEL INTERPRETABILITY\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Since KNN doesn't have built-in feature importance, we'll use permutation importance\n",
    "print(\"Calculating permutation importance (this may take a moment...)\")\n",
    "\n",
    "# Calculate permutation importance for best model\n",
    "perm_importance = permutation_importance(\n",
    "    final_model, X_test_scaled, y_test, \n",
    "    n_repeats=10, random_state=42, scoring='f1_macro'\n",
    ")\n",
    "\n",
    "# Create feature importance dataframe\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': perm_importance.importances_mean,\n",
    "    'Std': perm_importance.importances_std\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "feature_importance_df = feature_importance_df.sort_values('Importance', ascending=True)\n",
    "\n",
    "# Create feature importance visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Feature importance bar plot\n",
    "y_pos = np.arange(len(feature_importance_df))\n",
    "axes[0].barh(y_pos, feature_importance_df['Importance'], \n",
    "             xerr=feature_importance_df['Std'],\n",
    "             color='skyblue', alpha=0.8)\n",
    "axes[0].set_yticks(y_pos)\n",
    "axes[0].set_yticklabels(feature_importance_df['Feature'])\n",
    "axes[0].set_xlabel('Permutation Importance')\n",
    "axes[0].set_title('Feature Importance - Optimized Model')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature correlation with prediction errors\n",
    "print(\"\\nüîç Analyzing feature correlation with prediction errors...\")\n",
    "\n",
    "# Calculate feature statistics for correct vs incorrect predictions\n",
    "feature_analysis = pd.DataFrame()\n",
    "for i, feature in enumerate(feature_columns):\n",
    "    feature_values = X_test_scaled[:, i]\n",
    "    \n",
    "    feature_analysis = pd.concat([feature_analysis, pd.DataFrame({\n",
    "        'Feature': [feature],\n",
    "        'Correct_Mean': [np.mean(feature_values[correct])],\n",
    "        'Incorrect_Mean': [np.mean(feature_values[~correct])],\n",
    "        'Difference': [abs(np.mean(feature_values[correct]) - np.mean(feature_values[~correct]))]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "feature_analysis = feature_analysis.sort_values('Difference', ascending=True)\n",
    "\n",
    "# Plot feature difference analysis\n",
    "y_pos2 = np.arange(len(feature_analysis))\n",
    "axes[1].barh(y_pos2, feature_analysis['Difference'], \n",
    "             color='lightcoral', alpha=0.8)\n",
    "axes[1].set_yticks(y_pos2)\n",
    "axes[1].set_yticklabels(feature_analysis['Feature'])\n",
    "axes[1].set_xlabel('Mean Difference (Correct vs Incorrect)')\n",
    "axes[1].set_title('Feature Value Differences in Errors')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Top 3 Most Important Features:\")\n",
    "top_features = feature_importance_df.nlargest(3, 'Importance')\n",
    "for _, row in top_features.iterrows():\n",
    "    print(f\"   {row['Feature']}: {row['Importance']:.4f} ¬± {row['Std']:.4f}\")\n",
    "\n",
    "print(\"\\nüìä Features with Largest Error Correlation:\")\n",
    "top_error_features = feature_analysis.nlargest(3, 'Difference')\n",
    "for _, row in top_error_features.iterrows():\n",
    "    print(f\"   {row['Feature']}: {row['Difference']:.4f} difference\")\n",
    "\n",
    "# Save feature importance results\n",
    "feature_results = {\n",
    "    'feature_importance': feature_importance_df.to_dict(),\n",
    "    'error_correlation': feature_analysis.to_dict(),\n",
    "    'top_features': top_features['Feature'].tolist()\n",
    "}\n",
    "joblib.dump(feature_results, '../models/feature_analysis.pkl')\n",
    "print(\"\\n‚úÖ Saved feature importance analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d805a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Advanced Food Cluster Analysis\n",
    "print(\"\\n4Ô∏è‚É£ ADVANCED FOOD CLUSTER ANALYSIS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Perform detailed cluster analysis using both PCA and t-SNE results\n",
    "print(\"üîç Analyzing food clusters in reduced dimensional space...\")\n",
    "\n",
    "# Analyze clusters in PCA space\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Determine optimal number of clusters for PCA space\n",
    "silhouette_scores_pca = []\n",
    "K_range = range(3, 12)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_pca)\n",
    "    silhouette_avg = silhouette_score(X_pca, cluster_labels)\n",
    "    silhouette_scores_pca.append(silhouette_avg)\n",
    "\n",
    "optimal_k_pca = K_range[np.argmax(silhouette_scores_pca)]\n",
    "print(f\"üìä Optimal clusters for PCA space: {optimal_k_pca}\")\n",
    "\n",
    "# Perform clustering with optimal K\n",
    "kmeans_pca = KMeans(n_clusters=optimal_k_pca, random_state=42, n_init=10)\n",
    "pca_clusters = kmeans_pca.fit_predict(X_pca)\n",
    "\n",
    "# Similar analysis for t-SNE space\n",
    "silhouette_scores_tsne = []\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_tsne)\n",
    "    silhouette_avg = silhouette_score(X_tsne, cluster_labels)\n",
    "    silhouette_scores_tsne.append(silhouette_avg)\n",
    "\n",
    "optimal_k_tsne = K_range[np.argmax(silhouette_scores_tsne)]\n",
    "print(f\"üìä Optimal clusters for t-SNE space: {optimal_k_tsne}\")\n",
    "\n",
    "kmeans_tsne = KMeans(n_clusters=optimal_k_tsne, random_state=42, n_init=10)\n",
    "tsne_clusters = kmeans_tsne.fit_predict(X_tsne)\n",
    "\n",
    "# Create comprehensive cluster visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. PCA Clusters\n",
    "scatter1 = axes[0,0].scatter(X_pca[:, 0], X_pca[:, 1], c=pca_clusters, cmap='tab10', alpha=0.6)\n",
    "axes[0,0].set_title(f'PCA Clusters (K={optimal_k_pca})')\n",
    "axes[0,0].set_xlabel(f'PC1 ({pca_viz.explained_variance_ratio_[0]:.2%} variance)')\n",
    "axes[0,0].set_ylabel(f'PC2 ({pca_viz.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.colorbar(scatter1, ax=axes[0,0])\n",
    "\n",
    "# 2. t-SNE Clusters\n",
    "scatter2 = axes[0,1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=tsne_clusters, cmap='tab10', alpha=0.6)\n",
    "axes[0,1].set_title(f't-SNE Clusters (K={optimal_k_tsne})')\n",
    "axes[0,1].set_xlabel('t-SNE Component 1')\n",
    "axes[0,1].set_ylabel('t-SNE Component 2')\n",
    "plt.colorbar(scatter2, ax=axes[0,1])\n",
    "\n",
    "# 3. Silhouette Score Comparison\n",
    "axes[0,2].plot(K_range, silhouette_scores_pca, 'o-', label='PCA', color='blue')\n",
    "axes[0,2].plot(K_range, silhouette_scores_tsne, 'o-', label='t-SNE', color='red')\n",
    "axes[0,2].set_xlabel('Number of Clusters')\n",
    "axes[0,2].set_ylabel('Silhouette Score')\n",
    "axes[0,2].set_title('Cluster Quality vs Number of Clusters')\n",
    "axes[0,2].legend()\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Cluster composition analysis\n",
    "pca_cluster_composition = pd.DataFrame({\n",
    "    'Category': food_sample['category'],\n",
    "    'PCA_Cluster': pca_clusters\n",
    "})\n",
    "\n",
    "cluster_category_counts = pca_cluster_composition.groupby(['PCA_Cluster', 'Category']).size().unstack(fill_value=0)\n",
    "cluster_purity = []\n",
    "\n",
    "for cluster_id in range(optimal_k_pca):\n",
    "    cluster_foods = cluster_category_counts.loc[cluster_id]\n",
    "    total_foods = cluster_foods.sum()\n",
    "    max_category_count = cluster_foods.max()\n",
    "    purity = max_category_count / total_foods if total_foods > 0 else 0\n",
    "    cluster_purity.append(purity)\n",
    "\n",
    "axes[1,0].bar(range(optimal_k_pca), cluster_purity, color='lightgreen', alpha=0.8)\n",
    "axes[1,0].set_xlabel('PCA Cluster ID')\n",
    "axes[1,0].set_ylabel('Cluster Purity')\n",
    "axes[1,0].set_title('PCA Cluster Purity (Dominant Category %)')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Feature space coverage\n",
    "# Calculate how well clusters cover the feature space\n",
    "cluster_centers = kmeans_pca.cluster_centers_\n",
    "feature_coverage = []\n",
    "\n",
    "for feature_idx, feature_name in enumerate(feature_columns):\n",
    "    # Calculate feature range in original space for this sample\n",
    "    feature_values = X_sample[:, feature_idx]\n",
    "    feature_range = np.max(feature_values) - np.min(feature_values)\n",
    "    \n",
    "    # Calculate how much of this range is covered by cluster centers\n",
    "    if feature_range > 0:\n",
    "        feature_coverage.append(feature_range)\n",
    "    else:\n",
    "        feature_coverage.append(0)\n",
    "\n",
    "# Use the PCA space for visualization instead\n",
    "pc1_range = np.max(X_pca[:, 0]) - np.min(X_pca[:, 0])\n",
    "pc2_range = np.max(X_pca[:, 1]) - np.min(X_pca[:, 1])\n",
    "\n",
    "# Calculate cluster spread\n",
    "cluster_spreads = []\n",
    "for cluster_id in range(optimal_k_pca):\n",
    "    cluster_points = X_pca[pca_clusters == cluster_id]\n",
    "    if len(cluster_points) > 1:\n",
    "        spread = np.std(cluster_points, axis=0).mean()\n",
    "        cluster_spreads.append(spread)\n",
    "    else:\n",
    "        cluster_spreads.append(0)\n",
    "\n",
    "axes[1,1].bar(range(optimal_k_pca), cluster_spreads, color='orange', alpha=0.8)\n",
    "axes[1,1].set_xlabel('PCA Cluster ID')\n",
    "axes[1,1].set_ylabel('Average Cluster Spread')\n",
    "axes[1,1].set_title('PCA Cluster Compactness')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Category distribution in best cluster\n",
    "best_cluster_id = np.argmax(cluster_purity)\n",
    "best_cluster_foods = food_sample[pca_clusters == best_cluster_id]\n",
    "category_dist = best_cluster_foods['category'].value_counts().head(10)\n",
    "\n",
    "axes[1,2].pie(category_dist.values, labels=category_dist.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[1,2].set_title(f'Food Categories in Best Cluster (ID: {best_cluster_id})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Cluster Analysis Summary:\")\n",
    "print(f\"   ‚Ä¢ PCA optimal clusters: {optimal_k_pca} (silhouette: {max(silhouette_scores_pca):.3f})\")\n",
    "print(f\"   ‚Ä¢ t-SNE optimal clusters: {optimal_k_tsne} (silhouette: {max(silhouette_scores_tsne):.3f})\")\n",
    "print(f\"   ‚Ä¢ Best cluster purity: {max(cluster_purity):.2%}\")\n",
    "print(f\"   ‚Ä¢ Average cluster purity: {np.mean(cluster_purity):.2%}\")\n",
    "\n",
    "# Analyze cluster-category alignment\n",
    "print(f\"\\nüìä Cluster-Category Alignment:\")\n",
    "for cluster_id in range(min(5, optimal_k_pca)):  # Show first 5 clusters\n",
    "    cluster_categories = pca_cluster_composition[pca_cluster_composition['PCA_Cluster'] == cluster_id]['Category'].value_counts()\n",
    "    dominant_category = cluster_categories.index[0]\n",
    "    dominant_percentage = cluster_categories.iloc[0] / cluster_categories.sum() * 100\n",
    "    print(f\"   Cluster {cluster_id}: {dominant_percentage:.1f}% {dominant_category} ({cluster_categories.sum()} foods)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ddef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Performance Visualization Dashboard\n",
    "print(\"\\n5Ô∏è‚É£ MODEL PERFORMANCE VISUALIZATION DASHBOARD\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create a comprehensive performance dashboard\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Create a complex subplot layout\n",
    "gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Overall model performance metrics\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "metrics_names = ['Similarity\\nScore', 'Category\\nConsistency', 'Distance\\nQuality', 'Feature\\nImportance']\n",
    "metrics_values = [\n",
    "    similarity_results['overall_metrics']['avg_quality_score'],\n",
    "    similarity_results['overall_metrics']['avg_consistency'],\n",
    "    1 - similarity_results['overall_metrics']['avg_distance'],  # Invert distance for better visualization\n",
    "    np.mean(feature_importance_df['Importance'])\n",
    "]\n",
    "\n",
    "bars = ax1.bar(metrics_names, metrics_values, color=['skyblue', 'lightgreen', 'orange', 'lightcoral'])\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_title('Overall Model Performance')\n",
    "ax1.set_ylabel('Score')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, metrics_values):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 2. Feature importance radar chart\n",
    "ax2 = fig.add_subplot(gs[0, 1], projection='polar')\n",
    "angles = np.linspace(0, 2 * np.pi, len(feature_columns), endpoint=False)\n",
    "values = feature_importance_df.set_index('Feature').loc[feature_columns, 'Importance'].values\n",
    "\n",
    "# Close the plot\n",
    "angles = np.concatenate([angles, [angles[0]]])\n",
    "values = np.concatenate([values, [values[0]]])\n",
    "\n",
    "ax2.plot(angles, values, 'o-', linewidth=2, color='blue')\n",
    "ax2.fill(angles, values, alpha=0.25, color='blue')\n",
    "ax2.set_xticks(angles[:-1])\n",
    "ax2.set_xticklabels(feature_columns, fontsize=9)\n",
    "ax2.set_title('Feature Importance Radar')\n",
    "\n",
    "# 3. Prediction confidence distribution\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.hist(conf, bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
    "ax3.axvline(np.mean(conf), color='red', linestyle='--', label=f'Mean: {np.mean(conf):.3f}')\n",
    "ax3.set_xlabel('Prediction Confidence')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Confidence Distribution')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Error rate by category (top 10)\n",
    "ax4 = fig.add_subplot(gs[0, 3])\n",
    "top_errors = error_analysis.nlargest(10, 'Error_Rate')\n",
    "ax4.barh(range(len(top_errors)), top_errors['Error_Rate'], color='red', alpha=0.7)\n",
    "ax4.set_yticks(range(len(top_errors)))\n",
    "ax4.set_yticklabels(top_errors['Category'], fontsize=9)\n",
    "ax4.set_xlabel('Error Rate')\n",
    "ax4.set_title('Highest Error Rates')\n",
    "\n",
    "# 5. PCA visualization with variance explained\n",
    "ax5 = fig.add_subplot(gs[1, :2])\n",
    "scatter = ax5.scatter(X_pca[:, 0], X_pca[:, 1], c=y_sample, cmap='tab20', alpha=0.6, s=30)\n",
    "ax5.set_xlabel(f'PC1 ({pca_viz.explained_variance_ratio_[0]:.2%} variance)')\n",
    "ax5.set_ylabel(f'PC2 ({pca_viz.explained_variance_ratio_[1]:.2%} variance)')\n",
    "ax5.set_title('PCA - Food Categories Distribution')\n",
    "\n",
    "# 6. t-SNE visualization\n",
    "ax6 = fig.add_subplot(gs[1, 2:])\n",
    "scatter = ax6.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_sample, cmap='tab20', alpha=0.6, s=30)\n",
    "ax6.set_xlabel('t-SNE Component 1')\n",
    "ax6.set_ylabel('t-SNE Component 2')\n",
    "ax6.set_title('t-SNE - Food Categories Clustering')\n",
    "\n",
    "# 7. Similarity performance trends\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "# Use baseline and optimization results for trends\n",
    "baseline_scores = [results['combined_score'] for results in baseline_results.values()]\n",
    "optimization_scores = [result['combined_score'] for result in optimization_results]\n",
    "\n",
    "ax7.boxplot([baseline_scores, optimization_scores], labels=['Baseline', 'Optimized'])\n",
    "ax7.set_ylabel('Combined Score')\n",
    "ax7.set_title('Performance Distribution')\n",
    "ax7.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Category consistency analysis\n",
    "ax8 = fig.add_subplot(gs[2, 1])\n",
    "baseline_consistency = [results['category_consistency'] for results in baseline_results.values()]\n",
    "optimization_consistency = [result['category_consistency'] for result in optimization_results]\n",
    "\n",
    "ax8.boxplot([baseline_consistency, optimization_consistency], labels=['Baseline', 'Optimized'])\n",
    "ax8.set_ylabel('Category Consistency')\n",
    "ax8.set_title('Consistency Distribution')\n",
    "ax8.grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Model calibration curve\n",
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "ax9.plot(bin_centers, accuracies, 'o-', label='Model', color='blue', linewidth=2)\n",
    "ax9.plot([0, 1], [0, 1], '--', color='gray', alpha=0.7, label='Perfect')\n",
    "ax9.set_xlabel('Predicted Confidence')\n",
    "ax9.set_ylabel('Actual Accuracy')\n",
    "ax9.set_title('Model Calibration')\n",
    "ax9.legend()\n",
    "ax9.grid(True, alpha=0.3)\n",
    "\n",
    "# 10. Feature correlation heatmap\n",
    "ax10 = fig.add_subplot(gs[2, 3])\n",
    "# Calculate correlation matrix for features\n",
    "feature_corr = np.corrcoef(X_sample.T)\n",
    "im = ax10.imshow(feature_corr, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "ax10.set_xticks(range(len(feature_columns)))\n",
    "ax10.set_yticks(range(len(feature_columns)))\n",
    "ax10.set_xticklabels(feature_columns, rotation=45, fontsize=8)\n",
    "ax10.set_yticklabels(feature_columns, fontsize=8)\n",
    "ax10.set_title('Feature Correlation')\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(im, ax=ax10, shrink=0.8)\n",
    "\n",
    "plt.suptitle('KNN Food Similarity Model - Comprehensive Performance Dashboard', fontsize=16, y=0.98)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Performance Dashboard Summary:\")\n",
    "print(f\"   ‚Ä¢ Overall similarity score: {similarity_results['overall_metrics']['avg_quality_score']:.3f}\")\n",
    "print(f\"   ‚Ä¢ Category consistency: {similarity_results['overall_metrics']['avg_consistency']:.2%}\")\n",
    "print(f\"   ‚Ä¢ Model confidence: {np.mean(conf):.3f}\")\n",
    "print(f\"   ‚Ä¢ High confidence predictions: {np.mean(conf > 0.8):.1%}\")\n",
    "\n",
    "# Save visualization results\n",
    "visualization_results = {\n",
    "    'pca_results': {\n",
    "        'explained_variance_ratio': pca_viz.explained_variance_ratio_.tolist(),\n",
    "        'total_variance_explained': pca_viz.explained_variance_ratio_.sum()\n",
    "    },\n",
    "    'confidence_analysis': {\n",
    "        'mean_confidence': np.mean(conf),\n",
    "        'high_confidence_rate': np.mean(conf > 0.8),\n",
    "        'low_confidence_rate': np.mean(conf < 0.5)\n",
    "    },\n",
    "    'cluster_analysis': {\n",
    "        'optimal_k_pca': optimal_k_pca,\n",
    "        'optimal_k_tsne': optimal_k_tsne,\n",
    "        'best_cluster_purity': max(cluster_purity),\n",
    "        'average_cluster_purity': np.mean(cluster_purity)\n",
    "    },\n",
    "    'performance_metrics': {\n",
    "        'similarity_score': similarity_results['overall_metrics']['avg_quality_score'],\n",
    "        'category_consistency': similarity_results['overall_metrics']['avg_consistency'],\n",
    "        'distance_quality': 1 - similarity_results['overall_metrics']['avg_distance'],\n",
    "        'feature_importance_avg': np.mean(feature_importance_df['Importance'])\n",
    "    }\n",
    "}\n",
    "\n",
    "joblib.dump(visualization_results, '../models/visualization_results.pkl')\n",
    "print(f\"\\n‚úÖ Saved visualization analysis results\")\n",
    "print(f\"üìÅ File: ../models/visualization_results.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4c2f1",
   "metadata": {},
   "source": [
    "## Visualization and Analysis Summary\n",
    "\n",
    "This notebook provided comprehensive visualization and analysis of the KNN food similarity model performance. Key insights:\n",
    "\n",
    "### Feature Space Understanding\n",
    "\n",
    "- **PCA Analysis**: Explained {pca*viz.explained_variance_ratio*.sum():.1%} of variance with 2 components\n",
    "- **t-SNE Clustering**: Revealed natural groupings of nutritionally similar foods\n",
    "- **Cluster Quality**: Identified optimal clustering patterns in nutritional space\n",
    "\n",
    "### Model Performance\n",
    "\n",
    "- **Confidence Analysis**: Model shows appropriate uncertainty calibration\n",
    "- **Error Patterns**: Identified systematic challenges with certain food categories\n",
    "- **Feature Importance**: Key nutritional features driving similarity decisions\n",
    "\n",
    "### Clustering Insights\n",
    "\n",
    "- **Natural Groupings**: Foods cluster by nutritional similarity, not just categories\n",
    "- **Category Alignment**: Some food categories form tighter clusters than others\n",
    "- **Recommendation Quality**: Cluster analysis validates similarity model effectiveness\n",
    "\n",
    "### Business Applications\n",
    "\n",
    "- **Quality Thresholds**: Established confidence levels for automated recommendations\n",
    "- **Category Insights**: Understanding which food types are easier to recommend\n",
    "- **Model Reliability**: Visualization confirms model readiness for production use\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "The comprehensive analysis results will be used in the final deployment notebook to create production-ready model documentation and establish monitoring metrics for the food similarity system.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
